{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fb3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import time, sys, copy\n",
    "\n",
    "from enum import Enum\n",
    "from metrics import AUROC, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128d80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluatorMetrics(Enum):\n",
    "    AUROC = 'AUROC'\n",
    "    NDCG  = 'NDCG'\n",
    "    HR    = 'HR'\n",
    "\n",
    "def _create_empty_metrics_dict(cutoff_list, n_items, n_users, URM_train, URM_test):\n",
    "    empty_dict = {}\n",
    "    \n",
    "    for cutoff in cutoff_list:\n",
    "        cutoff_dict ={}\n",
    "        \n",
    "        for metric in EvaluatorMetrics:\n",
    "            cutoff_dict[metric.value] = 0.0\n",
    "            \n",
    "        empty_dict[cutoff] = cutoff_dict\n",
    "    return empty_dict\n",
    "\n",
    "def _remove_item_interactions(URM, item_list):\n",
    "\n",
    "    URM = sps.csc_matrix(URM.copy())\n",
    "\n",
    "    for item_index in item_list:\n",
    "\n",
    "        start_pos = URM.indptr[item_index]\n",
    "        end_pos = URM.indptr[item_index+1]\n",
    "\n",
    "        URM.data[start_pos:end_pos] = np.zeros_like(URM.data[start_pos:end_pos])\n",
    "\n",
    "    URM.eliminate_zeros()\n",
    "    URM = sps.csr_matrix(URM)\n",
    "\n",
    "    return URM\n",
    "\n",
    "class Evaluator(object):\n",
    "    EVALUATOR_NAME='EVALUATOR_BASE_CLASS'\n",
    "    \n",
    "    def __init__(self, URM_test_list, cutoff_list, exclude_seen=True,\n",
    "                 verbose=True):\n",
    "\n",
    "        self.cutoff_list = cutoff_list.copy()\n",
    "        self.max_cutoff = max(self.cutoff_list)\n",
    "\n",
    "        self.exclude_seen = exclude_seen\n",
    "\n",
    "        if not isinstance(URM_test_list, list):\n",
    "            self.URM_test = URM_test_list.copy()\n",
    "            URM_test_list = [URM_test_list]\n",
    "        else:\n",
    "            raise ValueError(\"List of URM_test not supported\")\n",
    "\n",
    "        self.n_users, self.n_items = URM_test_list[0].shape\n",
    "        \n",
    "        users_to_evaluate_mask = np.ones(self.n_users, dtype=np.bool)\n",
    "        \n",
    "        self.users_to_evaluate = np.arange(self.n_users)[users_to_evaluate_mask]\n",
    "        \n",
    "        self.users_to_evaluate = list(self.users_to_evaluate)\n",
    "\n",
    "        # Those will be set at each new evaluation\n",
    "        self._start_time = np.nan\n",
    "        self._start_time_print = np.nan\n",
    "        self._n_users_evaluated = np.nan\n",
    "        \n",
    "    def _print(self, string):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"{}: {}\".format(self.EVALUATOR_NAME, string))\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        self._start_time = time.time()\n",
    "        self._start_time_print = time.time()\n",
    "        self._n_users_evaluated = 0\n",
    "\n",
    "        results_dict = self._run_evaluation_on_selected_users(recommender_object, self.users_to_evaluate)\n",
    "        \n",
    "        if self._n_users_evaluated >0:\n",
    "            for cutoff in self.cutoff_list:\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "                \n",
    "                for key in results_current_cutoff.keys():\n",
    "                    value =results_current_cutoff[key]\n",
    "                    \n",
    "                    if isinstance(value, _Metrics_Object):\n",
    "                        results_current_cutoff[key] = value.get_metric_value()\n",
    "                    else:\n",
    "                        results_current_cutoff[key] = value/self._n_users_evaluated\n",
    "        else:\n",
    "            self._print(\"WARNING: No users had a sufficient number of relevant items\")\n",
    "            \n",
    "        return results_dict\n",
    "    \n",
    "    \n",
    "    def get_user_relevant_items(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in getting relevant items\"\n",
    "\n",
    "        return self.URM_test.indices[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id+1]]\n",
    "\n",
    "\n",
    "    def get_user_test_ratings(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in relevant items ratings\"\n",
    "\n",
    "        return self.URM_test.data[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id+1]]\n",
    "\n",
    "\n",
    "    def _compute_metrics_on_recommendation_list(self, test_user_batch_array, recommended_items_batch_list, scores_batch, results_dict):\n",
    "\n",
    "        assert len(recommended_items_batch_list) == len(test_user_batch_array), \"{}: recommended_items_batch_list contained recommendations for {} users, expected was {}\".format(\n",
    "            self.EVALUATOR_NAME, len(recommended_items_batch_list), len(test_user_batch_array))\n",
    "\n",
    "        assert scores_batch.shape[0] == len(test_user_batch_array), \"{}: scores_batch contained scores for {} users, expected was {}\".format(\n",
    "            self.EVALUATOR_NAME, scores_batch.shape[0], len(test_user_batch_array))\n",
    "\n",
    "        assert scores_batch.shape[1] == self.n_items, \"{}: scores_batch contained scores for {} items, expected was {}\".format(\n",
    "            self.EVALUATOR_NAME, scores_batch.shape[1], self.n_items)\n",
    "\n",
    "\n",
    "        # Compute recommendation quality for each user in batch\n",
    "        for batch_user_index in range(len(recommended_items_batch_list)):\n",
    "\n",
    "            test_user = test_user_batch_array[batch_user_index]\n",
    "\n",
    "            relevant_items = self.get_user_relevant_items(test_user)\n",
    "\n",
    "            # Being the URM CSR, the indices are the non-zero column indexes\n",
    "            recommended_items = recommended_items_batch_list[batch_user_index]\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "            self._n_users_evaluated += 1\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                is_relevant_current_cutoff = is_relevant[0:cutoff]\n",
    "                recommended_items_current_cutoff = recommended_items[0:cutoff]\n",
    "\n",
    "                results_current_cutoff[EvaluatorMetrics.ROC_AUC.value]              += roc_auc(is_relevant_current_cutoff)\n",
    "                results_current_cutoff[EvaluatorMetrics.NDCG.value]                 += ndcg(recommended_items_current_cutoff, relevant_items, relevance=self.get_user_test_ratings(test_user), at=cutoff)\n",
    "                results_current_cutoff[EvaluatorMetrics.HIT_RATE.value]             += is_relevant_current_cutoff.sum()\n",
    "\n",
    "\n",
    "        if time.time() - self._start_time_print > 30 or self._n_users_evaluated==len(self.users_to_evaluate):\n",
    "\n",
    "            elapsed_time = time.time()-self._start_time\n",
    "            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "            self._print(\"Processed {} ( {:.2f}% ) in {:.2f} {}. Users per second: {:.0f}\".format(\n",
    "                          self._n_users_evaluated,\n",
    "                          100.0* float(self._n_users_evaluated)/len(self.users_to_evaluate),\n",
    "                          new_time_value, new_time_unit,\n",
    "                          float(self._n_users_evaluated)/elapsed_time))\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stderr.flush()\n",
    "\n",
    "            self._start_time_print = time.time()\n",
    "\n",
    "        return results_dict\n",
    "\n",
    "\n",
    "\n",
    "class EvaluatorHoldout(Evaluator):\n",
    "    \"\"\"EvaluatorHoldout\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"EvaluatorHoldout\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, min_ratings_per_user=1, exclude_seen=True,\n",
    "                 verbose=True):\n",
    "\n",
    "\n",
    "        super(EvaluatorHoldout, self).__init__(URM_test_list, cutoff_list,\n",
    "                                               verbose = verbose)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _run_evaluation_on_selected_users(self, recommender_object, users_to_evaluate, block_size = None):\n",
    "\n",
    "        if block_size is None:\n",
    "            block_size = min(1000, int(1e8/self.n_items))\n",
    "            block_size = min(block_size, len(users_to_evaluate))\n",
    "\n",
    "\n",
    "        results_dict = _create_empty_metrics_dict(self.cutoff_list,\n",
    "                                                  self.n_items, self.n_users,\n",
    "                                                  recommender_object.get_URM_train(),\n",
    "                                                  self.URM_test)\n",
    "\n",
    "\n",
    "        # Start from -block_size to ensure it to be 0 at the first block\n",
    "        user_batch_start = 0\n",
    "        user_batch_end = 0\n",
    "\n",
    "        while user_batch_start < len(users_to_evaluate):\n",
    "\n",
    "            user_batch_end = user_batch_start + block_size\n",
    "            user_batch_end = min(user_batch_end, len(users_to_evaluate))\n",
    "\n",
    "            test_user_batch_array = np.array(users_to_evaluate[user_batch_start:user_batch_end])\n",
    "            user_batch_start = user_batch_end\n",
    "\n",
    "            # Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\n",
    "            recommended_items_batch_list, scores_batch = recommender_object.recommend(test_user_batch_array,\n",
    "                                                                      remove_seen_flag=self.exclude_seen,\n",
    "                                                                      cutoff = self.max_cutoff,\n",
    "                                                                      return_scores = True\n",
    "                                                                     )\n",
    "\n",
    "            results_dict = self._compute_metrics_on_recommendation_list(test_user_batch_array = test_user_batch_array,\n",
    "                                                         recommended_items_batch_list = recommended_items_batch_list,\n",
    "                                                         scores_batch = scores_batch,\n",
    "                                                         results_dict = results_dict)\n",
    "\n",
    "\n",
    "        return results_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8aa6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37env",
   "language": "python",
   "name": "dlevaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
